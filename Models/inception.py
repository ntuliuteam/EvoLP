import warnings

from typing import Any, Callable, List, Optional

import torch
import torch.nn.functional as F
from torch import nn, Tensor
import math


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor: Tensor, mean: float = 0., std: float = 1., a: float = -2., b: float = 2.) -> Tensor:
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class Inception3(nn.Module):
    def __init__(
            self,
            cfg=None,
            num_classes: int = 100,
            transform_input: bool = False,
            inception_blocks: Optional[List[Callable[..., nn.Module]]] = None,
            init_weights: Optional[bool] = None,
            dropout: float = 0.5
    ) -> None:
        super().__init__()
        # _log_api_usage_once(self)
        if inception_blocks is None:
            inception_blocks = [BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE]
        if init_weights is None:
            warnings.warn(
                "The default weight initialization of inception_v3 will be changed in future releases of "
                "torchvision. If you wish to keep the old behavior (which leads to long initialization times"
                " due to scipy/scipy#11299), please set init_weights=True.",
                FutureWarning,
            )
            init_weights = True
        if len(inception_blocks) != 6:
            raise ValueError(f"length of inception_blocks should be 6 instead of {len(inception_blocks)}")
        conv_block = inception_blocks[0]
        inception_a = inception_blocks[1]
        inception_b = inception_blocks[2]
        inception_c = inception_blocks[3]
        inception_d = inception_blocks[4]
        inception_e = inception_blocks[5]

        self.transform_input = transform_input
        self.Conv2d_1a_3x3 = conv_block(3, cfg[0], kernel_size=3, stride=2)  # cfg0 = 32
        self.Conv2d_2a_3x3 = conv_block(cfg[0], cfg[1], kernel_size=3)  # cfg1 = 32
        self.Conv2d_2b_3x3 = conv_block(cfg[1], cfg[2], kernel_size=3, padding=1)  # cfg2 = 64
        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)
        self.Conv2d_3b_1x1 = conv_block(cfg[2], cfg[3], kernel_size=1)  # cfg3=80
        self.Conv2d_4a_3x3 = conv_block(cfg[3], cfg[4], kernel_size=3)  # cfg4=192
        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)
        self.Mixed_5b = inception_a(cfg[4], cfg_a=cfg[5:12], pool_features=32)  # in cfg4 ; cfg5,6,7,8,9,10 pool=11
        _5b_out = self.Mixed_5b.out_channels
        assert _5b_out == cfg[5] + cfg[7] + cfg[10] + cfg[11]
        self.Mixed_5c = inception_a(_5b_out, cfg_a=cfg[12:19],
                                    pool_features=64)  # in cfg5,7,10,11; cfg12,13,14,15,16,17, pool=18
        _5c_out = self.Mixed_5c.out_channels
        assert _5c_out == cfg[12] + cfg[14] + cfg[17] + cfg[18]
        self.Mixed_5d = inception_a(_5c_out, cfg_a=cfg[19:26],
                                    pool_features=64)  # in cfg12,14,17,18; cfg19,20,21,22,23,24, pool=25
        _5d_out = self.Mixed_5d.out_channels
        assert _5d_out == cfg[19] + cfg[21] + cfg[24] + cfg[25]
        self.Mixed_6a = inception_b(_5d_out, cfg_b=cfg[26:30])  # in cfg19,21,24,25; cfg 26,27,28,29
        _6a_out = self.Mixed_6a.out_channels
        assert _6a_out == cfg[26] + cfg[29] + cfg[19] + cfg[21] + cfg[24] + cfg[25]
        self.Mixed_6b = inception_c(_6a_out, cfg_c=cfg[30:40], channels_7x7=128)  # cfg26,29 + cfg19,21,24,25; cfg 30-39
        _6b_out = self.Mixed_6b.out_channels
        assert _6b_out == cfg[30] + cfg[33] + cfg[38] + cfg[39]
        self.Mixed_6c = inception_c(_6b_out, cfg_c=cfg[40:50], channels_7x7=160)  # cfg30,33, 38, 39; cfg 40 -49
        _6c_out = self.Mixed_6c.out_channels
        assert _6c_out == cfg[40] + cfg[43] + cfg[48] + cfg[49]
        self.Mixed_6d = inception_c(_6c_out, cfg_c=cfg[50:60], channels_7x7=160)  # cfg 40, 43, 48, 49; cfg 50-59
        _6d_out = self.Mixed_6d.out_channels
        assert _6d_out == cfg[50] + cfg[53] + cfg[58] + cfg[59]
        self.Mixed_6e = inception_c(_6d_out, cfg_c=cfg[60:70], channels_7x7=192)  # cfg 50, 53, 58, 59; cfg 60 -69
        _6e_out = self.Mixed_6e.out_channels
        assert _6e_out == cfg[60] + cfg[63] + cfg[68] + cfg[69]
        self.Mixed_7a = inception_d(_6e_out, cfg_d=cfg[70:76])  # cfg60,63,68,69; cfg 70,71,72,73,74,75
        _7a_out = self.Mixed_7a.out_channels
        assert _7a_out == cfg[60] + cfg[63] + cfg[68] + cfg[69] + cfg[71] + cfg[75]
        self.Mixed_7b = inception_e(_7a_out, cfg_e=cfg[76:85])  # cfg60,63,68,69,71,75; cfg 76,77,78,79,80,81,82,83,84
        _7b_out = self.Mixed_7b.out_channels
        assert _7b_out == cfg[76] + cfg[78] + cfg[79] + cfg[82] + cfg[83] + cfg[84]
        self.Mixed_7c = inception_e(_7b_out, cfg_e=cfg[85:94])  # cfg76,78,79,82,83,84; cfg 85, 86, 87,88,89,90,91,92,93
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(p=dropout)
        _7c_out = self.Mixed_7c.out_channels
        assert _7c_out == cfg[85] + cfg[87] + cfg[88] + cfg[91] + cfg[92] + cfg[93]
        self.fc = nn.Linear(_7c_out, num_classes)  # cfg85,87,88,91,92,93
        if init_weights:
            for m in self.modules():
                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                    stddev = float(m.stddev) if hasattr(m, "stddev") else 0.1  # type: ignore
                    trunc_normal_(m.weight, mean=0.0, std=stddev, a=-2, b=2)
                elif isinstance(m, nn.BatchNorm2d):
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)

    def _transform_input(self, x: Tensor) -> Tensor:
        if self.transform_input:
            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5
            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5
            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5
            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)
        return x

    def _forward(self, x: Tensor):
        # N x 3 x 299 x 299
        x = self.Conv2d_1a_3x3(x)
        # N x 32 x 149 x 149
        x = self.Conv2d_2a_3x3(x)
        # N x 32 x 147 x 147
        x = self.Conv2d_2b_3x3(x)
        # N x 64 x 147 x 147
        x = self.maxpool1(x)
        # N x 64 x 73 x 73
        x = self.Conv2d_3b_1x1(x)
        # N x 80 x 73 x 73
        x = self.Conv2d_4a_3x3(x)
        # N x 192 x 71 x 71
        x = self.maxpool2(x)
        # N x 192 x 35 x 35
        x = self.Mixed_5b(x)
        # N x 256 x 35 x 35
        x = self.Mixed_5c(x)
        # N x 288 x 35 x 35
        x = self.Mixed_5d(x)
        # N x 288 x 35 x 35
        x = self.Mixed_6a(x)
        # N x 768 x 17 x 17
        x = self.Mixed_6b(x)
        # N x 768 x 17 x 17
        x = self.Mixed_6c(x)
        # N x 768 x 17 x 17
        x = self.Mixed_6d(x)
        # N x 768 x 17 x 17
        x = self.Mixed_6e(x)
        # N x 768 x 17 x 17
        x = self.Mixed_7a(x)
        # N x 1280 x 8 x 8
        x = self.Mixed_7b(x)
        # N x 2048 x 8 x 8
        x = self.Mixed_7c(x)
        # N x 2048 x 8 x 8
        # Adaptive average pooling
        x = self.avgpool(x)
        # N x 2048 x 1 x 1
        x = self.dropout(x)
        # N x 2048 x 1 x 1
        x = torch.flatten(x, 1)
        # N x 2048
        x = self.fc(x)
        # N x 1000 (num_classes)
        return x
        # , aux

    def forward(self, x: Tensor):
        x = self._transform_input(x)
        x = self._forward(x)
        return x


class InceptionA(nn.Module):
    def __init__(
            self, in_channels: int, pool_features: int, cfg_a=None,
            conv_block: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super().__init__()
        if conv_block is None:
            conv_block = BasicConv2d
        if cfg_a is None:
            cfg_a = [64, 48, 64, 64, 96, 96, pool_features]

        self.branch1x1 = conv_block(in_channels, cfg_a[0], kernel_size=1)

        self.branch5x5_1 = conv_block(in_channels, cfg_a[1], kernel_size=1)
        self.branch5x5_2 = conv_block(cfg_a[1], cfg_a[2], kernel_size=5, padding=2)

        self.branch3x3dbl_1 = conv_block(in_channels, cfg_a[3], kernel_size=1)
        self.branch3x3dbl_2 = conv_block(cfg_a[3], cfg_a[4], kernel_size=3, padding=1)
        self.branch3x3dbl_3 = conv_block(cfg_a[4], cfg_a[5], kernel_size=3, padding=1)

        self.branch_pool = conv_block(in_channels, cfg_a[6], kernel_size=1)

        self.out_channels = cfg_a[0] + cfg_a[2] + cfg_a[5] + cfg_a[6]

    def _forward(self, x: Tensor) -> List[Tensor]:
        branch1x1 = self.branch1x1(x)

        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)

        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]
        return outputs

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._forward(x)
        return torch.cat(outputs, 1)


class InceptionB(nn.Module):
    def __init__(self, in_channels: int, cfg_b=None, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:
        super().__init__()
        if conv_block is None:
            conv_block = BasicConv2d

        if cfg_b is None:
            cfg_b = [384, 64, 96, 96]
        self.branch3x3 = conv_block(in_channels, cfg_b[0], kernel_size=3, stride=2)

        self.branch3x3dbl_1 = conv_block(in_channels, cfg_b[1], kernel_size=1)
        self.branch3x3dbl_2 = conv_block(cfg_b[1], cfg_b[2], kernel_size=3, padding=1)
        self.branch3x3dbl_3 = conv_block(cfg_b[2], cfg_b[3], kernel_size=3, stride=2)

        self.out_channels = cfg_b[0] + cfg_b[3] + in_channels

    def _forward(self, x: Tensor) -> List[Tensor]:
        branch3x3 = self.branch3x3(x)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)

        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)

        outputs = [branch3x3, branch3x3dbl, branch_pool]
        return outputs

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._forward(x)
        return torch.cat(outputs, 1)


class InceptionC(nn.Module):
    def __init__(
            self, in_channels: int, channels_7x7: int, cfg_c=None, conv_block: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super().__init__()
        if conv_block is None:
            conv_block = BasicConv2d

        c7 = channels_7x7

        if cfg_c is None:
            cfg_c = [192, c7, c7, 192, c7, c7, c7, c7, 192, 192]

        self.branch1x1 = conv_block(in_channels, cfg_c[0], kernel_size=1)

        self.branch7x7_1 = conv_block(in_channels, cfg_c[1], kernel_size=1)
        self.branch7x7_2 = conv_block(cfg_c[1], cfg_c[2], kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7_3 = conv_block(cfg_c[2], cfg_c[3], kernel_size=(7, 1), padding=(3, 0))

        self.branch7x7dbl_1 = conv_block(in_channels, cfg_c[4], kernel_size=1)
        self.branch7x7dbl_2 = conv_block(cfg_c[4], cfg_c[5], kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7dbl_3 = conv_block(cfg_c[5], cfg_c[6], kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7dbl_4 = conv_block(cfg_c[6], cfg_c[7], kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7dbl_5 = conv_block(cfg_c[7], cfg_c[8], kernel_size=(1, 7), padding=(0, 3))

        self.branch_pool = conv_block(in_channels, cfg_c[9], kernel_size=1)

        self.out_channels = cfg_c[0] + cfg_c[3] + cfg_c[8] + cfg_c[9]

    def _forward(self, x: Tensor) -> List[Tensor]:
        branch1x1 = self.branch1x1(x)

        branch7x7 = self.branch7x7_1(x)
        branch7x7 = self.branch7x7_2(branch7x7)
        branch7x7 = self.branch7x7_3(branch7x7)

        branch7x7dbl = self.branch7x7dbl_1(x)
        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)

        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]
        return outputs

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._forward(x)
        return torch.cat(outputs, 1)


class InceptionD(nn.Module):
    def __init__(self, in_channels: int, cfg_d=None, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:
        super().__init__()
        if conv_block is None:
            conv_block = BasicConv2d

        if cfg_d is None:
            cfg_d = [192, 320, 192, 192, 192, 192]

        self.branch3x3_1 = conv_block(in_channels, cfg_d[0], kernel_size=1)
        self.branch3x3_2 = conv_block(cfg_d[0], cfg_d[1], kernel_size=3, stride=2)

        self.branch7x7x3_1 = conv_block(in_channels, cfg_d[2], kernel_size=1)
        self.branch7x7x3_2 = conv_block(cfg_d[2], cfg_d[3], kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7x3_3 = conv_block(cfg_d[3], cfg_d[4], kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7x3_4 = conv_block(cfg_d[4], cfg_d[5], kernel_size=3, stride=2)

        self.out_channels = cfg_d[1] + cfg_d[5] + in_channels

    def _forward(self, x: Tensor) -> List[Tensor]:
        branch3x3 = self.branch3x3_1(x)
        branch3x3 = self.branch3x3_2(branch3x3)

        branch7x7x3 = self.branch7x7x3_1(x)
        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)
        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)
        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)

        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)
        outputs = [branch3x3, branch7x7x3, branch_pool]
        return outputs

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._forward(x)
        return torch.cat(outputs, 1)


class InceptionE(nn.Module):
    def __init__(self, in_channels: int, cfg_e=None, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:
        super().__init__()
        if conv_block is None:
            conv_block = BasicConv2d

        if cfg_e is None:
            cfg_e = [320, 384, 384, 384, 448, 384, 384, 384, 192]

        self.branch1x1 = conv_block(in_channels, cfg_e[0], kernel_size=1)

        self.branch3x3_1 = conv_block(in_channels, cfg_e[1], kernel_size=1)
        self.branch3x3_2a = conv_block(cfg_e[1], cfg_e[2], kernel_size=(1, 3), padding=(0, 1))
        self.branch3x3_2b = conv_block(cfg_e[1], cfg_e[3], kernel_size=(3, 1), padding=(1, 0))

        self.branch3x3dbl_1 = conv_block(in_channels, cfg_e[4], kernel_size=1)
        self.branch3x3dbl_2 = conv_block(cfg_e[4], cfg_e[5], kernel_size=3, padding=1)
        self.branch3x3dbl_3a = conv_block(cfg_e[5], cfg_e[6], kernel_size=(1, 3), padding=(0, 1))
        self.branch3x3dbl_3b = conv_block(cfg_e[5], cfg_e[7], kernel_size=(3, 1), padding=(1, 0))

        self.branch_pool = conv_block(in_channels, cfg_e[8], kernel_size=1)

        self.out_channels = cfg_e[0] + cfg_e[2] + cfg_e[3] + cfg_e[6] + cfg_e[7] + cfg_e[8]

    def _forward(self, x: Tensor) -> List[Tensor]:
        branch1x1 = self.branch1x1(x)

        branch3x3 = self.branch3x3_1(x)
        branch3x3 = [
            self.branch3x3_2a(branch3x3),
            self.branch3x3_2b(branch3x3),
        ]
        branch3x3 = torch.cat(branch3x3, 1)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = [
            self.branch3x3dbl_3a(branch3x3dbl),
            self.branch3x3dbl_3b(branch3x3dbl),
        ]
        branch3x3dbl = torch.cat(branch3x3dbl, 1)

        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]
        return outputs

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._forward(x)
        return torch.cat(outputs, 1)


class BasicConv2d(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)

    def forward(self, x: Tensor) -> Tensor:
        x = self.conv(x)
        x = self.bn(x)
        return F.relu(x, inplace=True)


def inception_v3(cfg=None, init_weight=None):
    if cfg is None:
        cfg = [32, 32, 64, 80, 192, 64, 48, 64, 64, 96, 96, 32, 64, 48, 64, 64, 96, 96, 64, 64, 48, 64, 64, 96, 96, 64,
               384, 64, 96, 96, 192, 128, 128, 192, 128, 128, 128, 128, 192, 192, 192, 160, 160, 192, 160, 160, 160,
               160, 192, 192, 192, 160, 160, 192, 160, 160, 160, 160, 192, 192, 192, 192, 192, 192, 192, 192, 192, 192,
               192, 192, 192, 320, 192, 192, 192, 192, 320, 384, 384, 384, 448, 384, 384, 384, 192, 320, 384, 384, 384,
               448, 384, 384, 384, 192]

    model = Inception3(cfg, init_weights=init_weight)
    return model
